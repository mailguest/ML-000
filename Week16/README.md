# 项目报告与学习总结

首先，baseline版本中我们可以看到对比原版基础上做了一点微调

![image](https://user-images.githubusercontent.com/1567844/118799484-5a719180-b8d1-11eb-993d-51905a0806e8.png)

所以第一想法就是先在这个微调的版本上再进行调节，看看是否能够超越。

![image](https://user-images.githubusercontent.com/1567844/118799131-f8b12780-b8d0-11eb-8f32-ad8f7f7e02e2.png)


## 机器学习
### 要求：

根据三种衍生变量构建的方法每个至少构建出一个衍生变量，并检验其结果。实现非深度学习至少三个模型并进行集成。


### 思路

我们采用lightGBM算法，用5KFold方法划分数据训练不同模型，根据投票来确定最后结果。

采用调节LightGBM参数的方法、构建衍生变量的方法、使用其他算法API以及集成的思路来提升效果。


### LightGBM调参
#### num_leaves: 128 -> 168

在 LightGBM 当中，最主要的参数为 num_leaves（叶子个数）除此之外，一些方法还提供树的深度作为控制。

一般来说用叶子个数控制要比用深度控制更细腻，也更有助于选择优质变量。

注意：叶子数量跟学习率高度相关

  * 一般来说，当树更复杂的时候，学习率要尽量减小
  * 尽可能控制在学习率可以增加较合理范畴达到最好

#### num_round: 100 -> 250

残差树的数目。

#### learning_rate: 3e-3 -> 0.0093

我们先把学习率先调高一些。

#### bagging_fraction: 0.8 -> 0.972 & feature_fraction: 0.6 -> 0.84

这两个参数都是为了降低过拟合的，bagging_fraction + bagging_freq参数必须同时设置。

feature_fraction参数来进行特征的子抽样，这个参数可以用来防止过拟合及提高训练速度。

bagging_fraction相当于subsample样本采样，可以使bagging更快的运行，同时也可以降拟合。bagging_freq默认0，表示bagging的频率，0意味着没有使用bagging。


### 参数筛选的随机性
每一次随机选取多少变量或观测来拟合

在 LightGBM 当中，这个参数称之为 bagging_fraction 以及 feature_fraction。

我们这里测试 feature_fraction 从0.5 - 0.9，bagging_fraction 从0.6 - 1.0。

![image](https://user-images.githubusercontent.com/1567844/118807708-3adf6680-b8db-11eb-9b6b-5367cf07ba1f.png)

最终测试后效果如下,未超越助教提供的效果但是超过基础baseline版本。


### 构建衍生变量

在课上我们已知三种衍生变量构建的方法理论

构造变量的三种方法：

  * 基于业务理解：更多的是通过多种角度出发，构建可能的变量
  * 基于常见的构建模式
  * 根据 EDA探索性数据分析 和 Bad Case 分析

最常见的模式：

  1. 尝试对一系列具有类似业务解释的变量进行构建
  2. 然后对于整体有效果的变量进行集中构建
  3. 在变量个数达到一定数量的时候，进行一定的变量选择

通过观察训练集发现在loan_status取不同值时，以下几个变量差别较大

  1. discrete_term_1_one_hot
  2. discrete_term_2_one_hot
  3. discrete_purpose_1_one_hot
  4. discrete_home_ownership_1_one_hot

构建两个新的变量进行训练得到如下截图的提升：

![image](https://user-images.githubusercontent.com/1567844/118816399-b85ba480-b8e4-11eb-822c-dd139facbf1d.png)


### 调用其他算法API

分别调用sklearn中的这三类算法库，可以看到效果都超越了baseline

#### base

 * base_score: 0.91626
 * fine_tuning_score: 0.91702

#### linear model

 * SGD：0.8639
 * LogisticRegression：0.91108

#### ensemble
 * RandomForest：0.9164
 * GradientBoosting：0.91772 GOOD
 * AdaBoost：0.91604
 * Voting：0.91814 GOOD

 * lightgbm：0.91768 GOOD
 * xgboost：0.91712 GOOD


### 机器学习集成

我们采用两种集成方法


#### Stacking

该方法是一种分层模型集成框架。

以两层为例，首先将数据集分成训练集和测试集，利用训练集训练得到多个初级学习器，然后用初级学习器对测试集进行预测，并将输出值作为下一阶段训练的输入值，最终的标签作为输出值，用于训练次级学习器（通常最后一级使用Logistic回归）。

由于两次所使用的训练数据不同，因此可以在一定程度上防止过拟合。

由于要进行多次训练，因此这种方法要求训练数据很多，为了防止发生划分训练集和测试集后，测试集比例过小，生成的次级学习器泛化性能不强的问题，通常在Stacking算法中会使用交叉验证法或留一法来进行训练。

<b>效果待处理验证……</b>


#### 投票融合法

<b>效果待处理验证……</b>


### 深度学习
#### 要求：

实现至少一种深度学习网络，并比较其效果；

检查是否可以将之进行集成

根据自己选择的深度学习网络，实现至少一中训练的 trick


#### 加速方法

##### 优化器

SGD （+Momentum）或Adam


##### 学习率调整

  * 学习率多半需要很低
  * 增加复杂上层网络时候要注意对于已经训练好的权重和随机初始化的权重采用不同的学习率
  * 后者实现注意 train 和 eval 的正确选择


##### 不同的训练阶段

一般来说，当模型快进入到收敛的阶段时候，需要减少模型的变化程度（进行微调）

其他目的，减少算力消耗 加大 Batch Size 选用更慢的优化器（SGD+Momentum/Lookahead+SGD+Momentum)


##### 损失函数

在一些情况下，自然的损失函数未必是最有效的 在一些时候，过早的引入损失函数只会使得训练更加困难


##### Ensemble集成

不同的初始化 Ensemble 效果更好； 一些文章表明增加惩罚使得不同网络的权重不同更好。


##### 数据扩充

两种数据扩充方式：增加合理的样本；增加噪声

不宜过多；过多会实际导致模型记住观测 不同数据扩充方法可以进行 ensemble


##### Encoder 拼接

一般应用与 CV 和 NLP 数据当中

将不同 encoder 进行（两两）拼接并训练

增加 ensemble 多样性

一般来说 encoder 架构/训练方式越不同，带来的增益越大


#### 深度学习网络+Adam

因为Colab本身内置Tensorflow与keras，所以使用keras搭建神经网络

<b>效果待处理验证……</b>


#### 残差网络
在上一版基础上加入残差网络

<b>效果待处理验证……</b>


#### 使用TabNet

使用TbaNet测试

<b>效果待处理验证……</b>


#### 深度网络集成

将神经网络加入到k近邻+lightGBM+逻辑回归集成

<b>效果待处理验证……</b>


## 学习总结

本次长达几个月的机器学习收获很多，仔细回想更不仅仅是在知识上的储备，系统的方法论以及如何去学习的思想使我受益匪浅。

动手实验和自己独立了解最新或者工业界常用的算法，真正尝试从业务角度出发去思考以及模型的创建、数学理论的推导以及统计学的重要性

由于每周学习任务比较紧张，经常力不从心，课程结束后依然需要反复翻阅学习笔记以及代码。

最后感谢助教以及同学们，在毕设中也参考了一些同学的代码与思路，由衷感谢王然老师领进机器学习之门，让我看到更大的世界。

